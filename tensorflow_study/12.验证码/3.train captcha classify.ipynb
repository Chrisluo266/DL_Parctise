{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tensorflow.contrib import slim\n",
    "from tensorflow.contrib.slim import nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# 字符集\n",
    "CHAR_SET = [str(i) for i in \"0123456789ABCDE\"]\n",
    "CHAR_SET_LEN = len(CHAR_SET)\n",
    "print(CHAR_SET_LEN)\n",
    "# 训练集大小\n",
    "TRAIN_NUM = 44892\n",
    "# 批次大小\n",
    "BATCH_SIZE = 256\n",
    "# 迭代次数\n",
    "EPOCHES = 40\n",
    "# 循环次数\n",
    "LOOP_TIMES = EPOCHES*TRAIN_NUM//BATCH_SIZE\n",
    "# tf文件\n",
    "TFRECORD_TRAIN_FILE = 'captcha/tfrecords/train.tfrecord'\n",
    "\n",
    "# 初始学习率\n",
    "LEARNING_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_decode(filename):\n",
    "    tf_queue = tf.train.string_input_producer([filename])\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(tf_queue)\n",
    "    features = tf.parse_single_example(serialized_example,\n",
    "                                       features={\n",
    "                                           'image/encoded':tf.FixedLenFeature([], tf.string),\n",
    "                                            'image/label0': tf.FixedLenFeature([], tf.int64),\n",
    "                                            'image/label1': tf.FixedLenFeature([], tf.int64),\n",
    "                                            'image/label2': tf.FixedLenFeature([], tf.int64),\n",
    "                                            'image/label3': tf.FixedLenFeature([], tf.int64),\n",
    "                                       })\n",
    "    image = tf.decode_raw(features['image/encoded'], tf.uint8)\n",
    "    image = tf.reshape(image,[224,224,1])\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.subtract(image, 0.5)\n",
    "    image = tf.multiply(image, 2.0)\n",
    "\n",
    "    label0 = tf.cast(features['image/label0'], tf.int32)\n",
    "    label1 = tf.cast(features['image/label1'], tf.int32)\n",
    "    label2 = tf.cast(features['image/label2'], tf.int32)\n",
    "    label3 = tf.cast(features['image/label3'], tf.int32)\n",
    "    return image, label0, label1, label2, label3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-889503a2732a>:2: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From F:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From F:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From F:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From F:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From F:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\input.py:202: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-3-889503a2732a>:3: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\n",
      "WARNING:tensorflow:From <ipython-input-4-c7eef7caf0e2>:8: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n"
     ]
    }
   ],
   "source": [
    "image_tensor, label0_tensor, label1_tensor, label2_tensor, label3_tensor = read_and_decode(TFRECORD_TRAIN_FILE)\n",
    "\n",
    "image_batch_tensor, label0_batch_tensor, label1_batch_tensor, label2_batch_tensor, label3_batch_tensor = tf.train.shuffle_batch(\n",
    "    [image_tensor, label0_tensor, label1_tensor, label2_tensor, label3_tensor],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    capacity=10000,\n",
    "    min_after_dequeue=2000,\n",
    "    num_threads=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From F:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From F:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# 网络 \n",
    "x = tf.placeholder(tf.float32,[None,224,224,1])\n",
    "y0 = tf.placeholder(tf.int32,[None])\n",
    "y1 = tf.placeholder(tf.int32,[None])\n",
    "y2 = tf.placeholder(tf.int32,[None])\n",
    "y3 = tf.placeholder(tf.int32,[None])\n",
    "lr = tf.Variable(LEARNING_RATE, dtype=tf.float32,trainable=False)\n",
    "\n",
    "with slim.arg_scope(nets.alexnet.alexnet_v2_arg_scope(weight_decay=0.0005)):\n",
    "    outputs, end_points = nets.alexnet.alexnet_v2(x,CHAR_SET_LEN,is_training=True,dropout_keep_prob=1)\n",
    "    #modify alexnet_v2\n",
    "    fc7 = tf.get_default_graph().get_tensor_by_name(\"alexnet_v2/fc7/Relu:0\")\n",
    "\n",
    "    logits0 = tf.contrib.layers.conv2d(\n",
    "            fc7,\n",
    "            CHAR_SET_LEN, [1, 1],\n",
    "            activation_fn=None,\n",
    "            normalizer_fn=None,\n",
    "            weights_initializer=tf.truncated_normal_initializer(0.0, 0.005),\n",
    "            biases_initializer=tf.zeros_initializer(),\n",
    "            scope='fc8-0')\n",
    "    logits1 = tf.contrib.layers.conv2d(\n",
    "            fc7,\n",
    "            CHAR_SET_LEN, [1, 1],\n",
    "            activation_fn=None,\n",
    "            normalizer_fn=None,\n",
    "            weights_initializer=tf.truncated_normal_initializer(0.0, 0.005),\n",
    "            biases_initializer=tf.zeros_initializer(),\n",
    "            scope='fc8-1')\n",
    "    logits2 = tf.contrib.layers.conv2d(\n",
    "            fc7,\n",
    "            CHAR_SET_LEN, [1, 1],\n",
    "            activation_fn=None,\n",
    "            normalizer_fn=None,\n",
    "            weights_initializer=tf.truncated_normal_initializer(0.0, 0.005),\n",
    "            biases_initializer=tf.zeros_initializer(),\n",
    "            scope='fc8-2')\n",
    "    logits3 = tf.contrib.layers.conv2d(\n",
    "            fc7,\n",
    "            CHAR_SET_LEN, [1, 1],\n",
    "            activation_fn=None,\n",
    "            normalizer_fn=None,\n",
    "            weights_initializer=tf.truncated_normal_initializer(0.0, 0.005),\n",
    "            biases_initializer=tf.zeros_initializer(),\n",
    "            scope='fc8-3')\n",
    "\n",
    "logits0 = tf.squeeze(logits0, [1, 2], name='fc8-0/squeezed')\n",
    "logits1 = tf.squeeze(logits1, [1, 2], name='fc8-1/squeezed')\n",
    "logits2 = tf.squeeze(logits2, [1, 2], name='fc8-2/squeezed')\n",
    "logits3 = tf.squeeze(logits3, [1, 2], name='fc8-3/squeezed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss0 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y0,logits=logits0))\n",
    "loss1 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y1,logits=logits1))\n",
    "loss2 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y2,logits=logits2))\n",
    "loss3 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y3,logits=logits3))\n",
    "\n",
    "total_loss = (loss0 + loss1 + loss2 + loss3)/4.0\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(total_loss)\n",
    "\n",
    "# 计算准确率\n",
    "correct_pre0 = tf.cast(tf.equal(y0, tf.argmax(logits0, 1,output_type=tf.int32)),tf.float32)\n",
    "accuracy0 = tf.reduce_mean(correct_pre0)\n",
    "\n",
    "correct_pre1 = tf.cast(tf.equal(y1, tf.argmax(logits1, 1,output_type=tf.int32)),tf.float32)\n",
    "accuracy1 = tf.reduce_mean(correct_pre1)\n",
    "\n",
    "correct_pre2 = tf.cast(tf.equal(y2, tf.argmax(logits2, 1,output_type=tf.int32)),tf.float32)\n",
    "accuracy2 = tf.reduce_mean(correct_pre2)\n",
    "\n",
    "correct_pre3 = tf.cast(tf.equal(y3, tf.argmax(logits3, 1,output_type=tf.int32)),tf.float32)\n",
    "accuracy3 = tf.reduce_mean(correct_pre3)\n",
    "\n",
    "accuracy = tf.reduce_mean(correct_pre0*correct_pre1*correct_pre2*correct_pre3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-c2bee0835009>:13: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "Iter:0/7014 epoch:1,    Loss:2.677    Accuracy:0.10,0.11,0.11,0.09     Total_Acc =0.0000   Learning_rate:0.00010000\n",
      "Iter:100/7014 epoch:1,    Loss:2.706    Accuracy:0.08,0.05,0.06,0.11     Total_Acc =0.0000   Learning_rate:0.00010000\n",
      "Iter:200/7014 epoch:2,    Loss:2.707    Accuracy:0.05,0.09,0.09,0.09     Total_Acc =0.0000   Learning_rate:0.00010000\n",
      "Iter:300/7014 epoch:2,    Loss:2.709    Accuracy:0.06,0.06,0.06,0.05     Total_Acc =0.0000   Learning_rate:0.00010000\n",
      "Iter:400/7014 epoch:3,    Loss:2.682    Accuracy:0.08,0.12,0.11,0.09     Total_Acc =0.0000   Learning_rate:0.00010000\n",
      "Iter:500/7014 epoch:3,    Loss:2.344    Accuracy:0.25,0.19,0.17,0.19     Total_Acc =0.0039   Learning_rate:0.00010000\n",
      "Iter:600/7014 epoch:4,    Loss:1.873    Accuracy:0.46,0.24,0.27,0.45     Total_Acc =0.0078   Learning_rate:0.00010000\n",
      "Iter:700/7014 epoch:4,    Loss:1.435    Accuracy:0.58,0.38,0.38,0.46     Total_Acc =0.0547   Learning_rate:0.00010000\n",
      "Iter:800/7014 epoch:5,    Loss:1.232    Accuracy:0.65,0.45,0.51,0.59     Total_Acc =0.0938   Learning_rate:0.00007500\n",
      "Iter:900/7014 epoch:6,    Loss:0.985    Accuracy:0.70,0.61,0.59,0.66     Total_Acc =0.1953   Learning_rate:0.00007500\n",
      "Iter:1000/7014 epoch:6,    Loss:0.864    Accuracy:0.78,0.61,0.64,0.73     Total_Acc =0.2539   Learning_rate:0.00007500\n",
      "Iter:1100/7014 epoch:7,    Loss:0.797    Accuracy:0.77,0.65,0.62,0.75     Total_Acc =0.2695   Learning_rate:0.00007500\n",
      "Iter:1200/7014 epoch:7,    Loss:0.782    Accuracy:0.79,0.71,0.67,0.75     Total_Acc =0.2969   Learning_rate:0.00007500\n",
      "Iter:1300/7014 epoch:8,    Loss:0.710    Accuracy:0.77,0.74,0.66,0.77     Total_Acc =0.3047   Learning_rate:0.00007500\n",
      "Iter:1400/7014 epoch:8,    Loss:0.606    Accuracy:0.83,0.74,0.74,0.80     Total_Acc =0.3828   Learning_rate:0.00007500\n",
      "Iter:1500/7014 epoch:9,    Loss:0.576    Accuracy:0.85,0.75,0.71,0.80     Total_Acc =0.3633   Learning_rate:0.00007500\n",
      "Iter:1600/7014 epoch:10,    Loss:0.478    Accuracy:0.85,0.80,0.78,0.85     Total_Acc =0.4844   Learning_rate:0.00005625\n",
      "Iter:1700/7014 epoch:10,    Loss:0.452    Accuracy:0.85,0.83,0.80,0.82     Total_Acc =0.4766   Learning_rate:0.00005625\n",
      "Iter:1800/7014 epoch:11,    Loss:0.469    Accuracy:0.88,0.82,0.77,0.83     Total_Acc =0.4844   Learning_rate:0.00005625\n",
      "Iter:1900/7014 epoch:11,    Loss:0.457    Accuracy:0.86,0.80,0.80,0.87     Total_Acc =0.4688   Learning_rate:0.00005625\n",
      "save model.   at 7014 steps\n",
      "Iter:2000/7014 epoch:12,    Loss:0.450    Accuracy:0.87,0.81,0.80,0.85     Total_Acc =0.5508   Learning_rate:0.00005625\n",
      "Iter:2100/7014 epoch:12,    Loss:0.378    Accuracy:0.88,0.85,0.83,0.93     Total_Acc =0.6016   Learning_rate:0.00005625\n",
      "Iter:2200/7014 epoch:13,    Loss:0.363    Accuracy:0.89,0.84,0.86,0.89     Total_Acc =0.5977   Learning_rate:0.00005625\n",
      "Iter:2300/7014 epoch:14,    Loss:0.348    Accuracy:0.90,0.84,0.84,0.91     Total_Acc =0.6133   Learning_rate:0.00005625\n",
      "Iter:2400/7014 epoch:14,    Loss:0.329    Accuracy:0.84,0.89,0.87,0.89     Total_Acc =0.5977   Learning_rate:0.00005625\n",
      "Iter:2500/7014 epoch:15,    Loss:0.335    Accuracy:0.89,0.87,0.81,0.93     Total_Acc =0.6172   Learning_rate:0.00004219\n",
      "Iter:2600/7014 epoch:15,    Loss:0.349    Accuracy:0.94,0.87,0.85,0.86     Total_Acc =0.6367   Learning_rate:0.00004219\n",
      "Iter:2700/7014 epoch:16,    Loss:0.325    Accuracy:0.93,0.89,0.82,0.90     Total_Acc =0.6172   Learning_rate:0.00004219\n",
      "Iter:2800/7014 epoch:16,    Loss:0.290    Accuracy:0.93,0.86,0.85,0.90     Total_Acc =0.6289   Learning_rate:0.00004219\n",
      "Iter:2900/7014 epoch:17,    Loss:0.285    Accuracy:0.91,0.90,0.86,0.93     Total_Acc =0.6719   Learning_rate:0.00004219\n",
      "Iter:3000/7014 epoch:18,    Loss:0.276    Accuracy:0.91,0.91,0.87,0.92     Total_Acc =0.6641   Learning_rate:0.00004219\n",
      "Iter:3100/7014 epoch:18,    Loss:0.264    Accuracy:0.89,0.90,0.86,0.93     Total_Acc =0.6523   Learning_rate:0.00004219\n",
      "Iter:3200/7014 epoch:19,    Loss:0.231    Accuracy:0.95,0.93,0.87,0.95     Total_Acc =0.7344   Learning_rate:0.00004219\n",
      "Iter:3300/7014 epoch:19,    Loss:0.234    Accuracy:0.94,0.93,0.90,0.93     Total_Acc =0.7266   Learning_rate:0.00004219\n",
      "Iter:3400/7014 epoch:20,    Loss:0.257    Accuracy:0.93,0.89,0.88,0.94     Total_Acc =0.7070   Learning_rate:0.00003164\n",
      "Iter:3500/7014 epoch:20,    Loss:0.250    Accuracy:0.95,0.90,0.85,0.93     Total_Acc =0.6680   Learning_rate:0.00003164\n",
      "Iter:3600/7014 epoch:21,    Loss:0.233    Accuracy:0.95,0.89,0.90,0.93     Total_Acc =0.7188   Learning_rate:0.00003164\n",
      "Iter:3700/7014 epoch:22,    Loss:0.250    Accuracy:0.96,0.88,0.88,0.93     Total_Acc =0.6758   Learning_rate:0.00003164\n",
      "Iter:3800/7014 epoch:22,    Loss:0.222    Accuracy:0.94,0.89,0.92,0.94     Total_Acc =0.7539   Learning_rate:0.00003164\n",
      "Iter:3900/7014 epoch:23,    Loss:0.202    Accuracy:0.95,0.92,0.92,0.94     Total_Acc =0.7617   Learning_rate:0.00003164\n",
      "save model.   at 7014 steps\n",
      "Iter:4000/7014 epoch:23,    Loss:0.236    Accuracy:0.93,0.89,0.90,0.95     Total_Acc =0.7070   Learning_rate:0.00003164\n",
      "Iter:4100/7014 epoch:24,    Loss:0.170    Accuracy:0.95,0.95,0.91,0.95     Total_Acc =0.7773   Learning_rate:0.00003164\n",
      "Iter:4200/7014 epoch:24,    Loss:0.217    Accuracy:0.96,0.94,0.91,0.91     Total_Acc =0.7461   Learning_rate:0.00003164\n",
      "Iter:4300/7014 epoch:25,    Loss:0.199    Accuracy:0.94,0.92,0.90,0.95     Total_Acc =0.7617   Learning_rate:0.00002373\n",
      "Iter:4400/7014 epoch:26,    Loss:0.218    Accuracy:0.94,0.92,0.90,0.94     Total_Acc =0.7383   Learning_rate:0.00002373\n",
      "Iter:4500/7014 epoch:26,    Loss:0.158    Accuracy:0.96,0.93,0.93,0.97     Total_Acc =0.7969   Learning_rate:0.00002373\n",
      "Iter:4600/7014 epoch:27,    Loss:0.183    Accuracy:0.96,0.95,0.91,0.95     Total_Acc =0.8008   Learning_rate:0.00002373\n",
      "Iter:4700/7014 epoch:27,    Loss:0.164    Accuracy:0.98,0.93,0.93,0.95     Total_Acc =0.7969   Learning_rate:0.00002373\n",
      "Iter:4800/7014 epoch:28,    Loss:0.159    Accuracy:0.96,0.94,0.92,0.96     Total_Acc =0.8047   Learning_rate:0.00002373\n",
      "Iter:4900/7014 epoch:28,    Loss:0.200    Accuracy:0.96,0.91,0.95,0.93     Total_Acc =0.7852   Learning_rate:0.00002373\n",
      "Iter:5000/7014 epoch:29,    Loss:0.141    Accuracy:0.97,0.95,0.94,0.95     Total_Acc =0.8359   Learning_rate:0.00002373\n",
      "Iter:5100/7014 epoch:30,    Loss:0.160    Accuracy:0.97,0.93,0.93,0.96     Total_Acc =0.8008   Learning_rate:0.00001780\n",
      "Iter:5200/7014 epoch:30,    Loss:0.140    Accuracy:0.97,0.93,0.93,0.95     Total_Acc =0.8047   Learning_rate:0.00001780\n",
      "Iter:5300/7014 epoch:31,    Loss:0.168    Accuracy:0.95,0.93,0.95,0.94     Total_Acc =0.7930   Learning_rate:0.00001780\n",
      "Iter:5400/7014 epoch:31,    Loss:0.164    Accuracy:0.97,0.93,0.91,0.96     Total_Acc =0.7969   Learning_rate:0.00001780\n",
      "Iter:5500/7014 epoch:32,    Loss:0.148    Accuracy:0.96,0.95,0.96,0.96     Total_Acc =0.8398   Learning_rate:0.00001780\n",
      "Iter:5600/7014 epoch:32,    Loss:0.135    Accuracy:0.96,0.96,0.95,0.95     Total_Acc =0.8359   Learning_rate:0.00001780\n",
      "Iter:5700/7014 epoch:33,    Loss:0.144    Accuracy:0.97,0.94,0.93,0.95     Total_Acc =0.8047   Learning_rate:0.00001780\n",
      "Iter:5800/7014 epoch:34,    Loss:0.137    Accuracy:0.98,0.94,0.94,0.98     Total_Acc =0.8594   Learning_rate:0.00001780\n",
      "Iter:5900/7014 epoch:34,    Loss:0.159    Accuracy:0.96,0.93,0.95,0.95     Total_Acc =0.8047   Learning_rate:0.00001780\n",
      "save model.   at 7014 steps\n",
      "Iter:6000/7014 epoch:35,    Loss:0.120    Accuracy:0.98,0.94,0.96,0.96     Total_Acc =0.8438   Learning_rate:0.00001335\n",
      "Iter:6100/7014 epoch:35,    Loss:0.130    Accuracy:0.98,0.93,0.94,0.98     Total_Acc =0.8516   Learning_rate:0.00001335\n",
      "Iter:6200/7014 epoch:36,    Loss:0.141    Accuracy:0.98,0.92,0.93,0.97     Total_Acc =0.8281   Learning_rate:0.00001335\n",
      "Iter:6300/7014 epoch:36,    Loss:0.103    Accuracy:0.99,0.97,0.95,0.97     Total_Acc =0.8828   Learning_rate:0.00001335\n",
      "Iter:6400/7014 epoch:37,    Loss:0.124    Accuracy:0.96,0.97,0.93,0.96     Total_Acc =0.8359   Learning_rate:0.00001335\n",
      "Iter:6500/7014 epoch:38,    Loss:0.134    Accuracy:0.98,0.95,0.93,0.97     Total_Acc =0.8438   Learning_rate:0.00001335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:6600/7014 epoch:38,    Loss:0.146    Accuracy:0.96,0.92,0.95,0.95     Total_Acc =0.8008   Learning_rate:0.00001335\n",
      "Iter:6700/7014 epoch:39,    Loss:0.149    Accuracy:0.98,0.94,0.93,0.95     Total_Acc =0.8359   Learning_rate:0.00001335\n",
      "Iter:6800/7014 epoch:39,    Loss:0.111    Accuracy:0.96,0.97,0.95,0.98     Total_Acc =0.8516   Learning_rate:0.00001335\n",
      "Iter:6900/7014 epoch:40,    Loss:0.119    Accuracy:0.97,0.95,0.96,0.97     Total_Acc =0.8633   Learning_rate:0.00001001\n",
      "Iter:7000/7014 epoch:40,    Loss:0.124    Accuracy:0.96,0.94,0.96,0.98     Total_Acc =0.8594   Learning_rate:0.00001001\n",
      "Save model at last.\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver(max_to_keep=3)\n",
    "\n",
    "CHECK_POINT_DIR = \"./captcha/model/\"\n",
    "\n",
    "latest_checkpoint = tf.train.latest_checkpoint(CHECK_POINT_DIR)\n",
    "update_lr = tf.assign(lr,lr*0.75)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    if latest_checkpoint != None:\n",
    "        saver.restore(sess,latest_checkpoint)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "    i_epoch = 0\n",
    "\n",
    "    for i in range(LOOP_TIMES):\n",
    "        b_image, b_label0, b_label1, b_label2, b_label3 = sess.run(\n",
    "            [image_batch_tensor, label0_batch_tensor, label1_batch_tensor, label2_batch_tensor, label3_batch_tensor])\n",
    "        sess.run(optimizer, feed_dict={x:b_image,\n",
    "                                       y0:b_label0,\n",
    "                                       y1:b_label1,\n",
    "                                       y2:b_label2,\n",
    "                                       y3:b_label3})\n",
    "        i_epoch_new = i//(TRAIN_NUM/BATCH_SIZE) + 1\n",
    "        if i_epoch != i_epoch_new:\n",
    "            i_epoch = i_epoch_new\n",
    "            if i_epoch%5 == 0:\n",
    "                sess.run(update_lr)\n",
    "\n",
    "        if i%100 == 0:\n",
    "            acc0, acc1, acc2, acc3,acc, loss_ = sess.run([accuracy0, accuracy1,accuracy2, accuracy3,accuracy,total_loss],\n",
    "                                                     feed_dict={x:b_image,\n",
    "                                                               y0:b_label0,\n",
    "                                                               y1:b_label1,\n",
    "                                                               y2:b_label2,\n",
    "                                                               y3:b_label3})\n",
    "            learning_rate = sess.run(lr)\n",
    "            print(\"Iter:%d/%d epoch:%d,    Loss:%.3f    Accuracy:%.2f,%.2f,%.2f,%.2f     Total_Acc =%.4f   Learning_rate:%.8f\" % (\n",
    "                i, LOOP_TIMES, i_epoch, loss_, acc0, acc1, acc2, acc3, acc, learning_rate))\n",
    "        if (i+1) % 2000 ==0:\n",
    "            print(\"save model.   at {} steps\".format(LOOP_TIMES))\n",
    "            saver.save(sess, CHECK_POINT_DIR + '/crack_captcha.model',global_step=LOOP_TIMES)\n",
    "            \n",
    "    print(\"Save model at last.\")\n",
    "    saver.save(sess, CHECK_POINT_DIR + '/crack_captcha.model',global_step=LOOP_TIMES)\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
