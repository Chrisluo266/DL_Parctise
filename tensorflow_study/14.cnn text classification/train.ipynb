{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV_SAMPLE_PERCENTAGE = 0.1\n",
    "POSITIVE_DATA_FILE = \"./data/rt-polaritydata/rt-polarity.pos\"\n",
    "NEGATIVE_DATA_FILE = \"./data/rt-polaritydata/rt-polarity.neg\"\n",
    "\n",
    "#model params\n",
    "EMBEDDING_DIM = 128\n",
    "FILTER_ROWS = [3,4,5,6]\n",
    "NUM_FILTERS = 64\n",
    "DROPOUT_KEEP_PROB = 0.5\n",
    "L2_REG_LAMBDA = 0.0005\n",
    "\n",
    "#trainning params\n",
    "BATCH_SIZE = 2048\n",
    "NUM_EPOCH = 300\n",
    "EVAL_STEPS = 100\n",
    "CHECKPOINT_STEPS = 200\n",
    "MAX_CHECK_POINTS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "WARNING:tensorflow:From <ipython-input-3-86109cd73aed>:11: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From F:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From F:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "x_shape: (10662, 56)\n",
      "y_shape: (10662, 2)\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x_text, y = data_helpers.load_data_and_labels(POSITIVE_DATA_FILE, NEGATIVE_DATA_FILE)\n",
    "\n",
    "# Build vocabulary\n",
    "# 一行数据最多的词汇数\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor( max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "print(\"x_shape:\",x.shape)\n",
    "print(\"y_shape:\",y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Vocabulary: 18758\n",
      "Train shape =  (9596, 56)\n",
      "Val shape =  (1066, 56)\n",
      "smaple x: [[ 4719    59   182    34   190   804     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [  129  7044   284   146    80     3  1116    58    84  1386   182  1968\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [  146     3   453    88    34     1   190  8338 18328    12  2320  1573\n",
      "   3840  6569 16227   112  1543   246    17  1722  5117     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [ 2718   149  7850   503   343    87  6515   250  2934     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]\n",
      " [  343   133     1  1528    34  2691   643 16667   125     1  4435    34\n",
      "    983   740     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]]\n",
      "smaple y: [[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "data_counts = len(y)\n",
    "\n",
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(data_counts))\n",
    "\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "# 数据集切分为两部分\n",
    "dev_sample_index = -1 * int(DEV_SAMPLE_PERCENTAGE * float(data_counts))\n",
    "x_train, x_val = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_val = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "print(\"Num of Vocabulary: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train shape = \",x_train.shape)\n",
    "print(\"Val shape = \",x_val.shape)\n",
    "\n",
    "print(\"smaple x:\",x_train[0:5])\n",
    "print(\"smaple y:\",y_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From F:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From F:\\JupyterRoot\\DL_Parctise\\tensorflow_study\\14.cnn text classification\\text_cnn.py:75: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From F:\\JupyterRoot\\DL_Parctise\\tensorflow_study\\14.cnn text classification\\text_cnn.py:92: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Writing to F:\\JupyterRoot\\DL_Parctise\\tensorflow_study\\14.cnn text classification\\runs\\1560327157\n",
      "\n",
      "WARNING:tensorflow:From F:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-6/W:0/grad/hist is illegal; using conv-maxpool-6/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-6/W:0/grad/sparsity is illegal; using conv-maxpool-6/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-6/b:0/grad/hist is illegal; using conv-maxpool-6/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-6/b:0/grad/sparsity is illegal; using conv-maxpool-6/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "text_cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=EMBEDDING_DIM,\n",
    "            filter_sizes=FILTER_ROWS,\n",
    "            num_filters=NUM_FILTERS,\n",
    "            l2_reg_lambda=L2_REG_LAMBDA)\n",
    "\n",
    "# Output directory for models and summaries\n",
    "# 定义输出路径\n",
    "timestamp = str(int(time.time()))\n",
    "out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "# Define Training procedure\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "# 计算梯度\n",
    "grads_and_vars = optimizer.compute_gradients(text_cnn.loss)\n",
    "train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "# 保存变量的梯度值\n",
    "grad_summaries = []\n",
    "for g, v in grads_and_vars:\n",
    "    if g is not None:\n",
    "        grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "        sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        grad_summaries.append(grad_hist_summary)\n",
    "        grad_summaries.append(sparsity_summary)\n",
    "        \n",
    "grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    " # Summaries for loss and accuracy\n",
    "loss_summary = tf.summary.scalar(\"loss\", text_cnn.loss)\n",
    "acc_summary = tf.summary.scalar(\"accuracy\", text_cnn.accuracy)\n",
    "\n",
    "# Train Summaries\n",
    "train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "\n",
    "# val summaries\n",
    "val_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "val_summary_dir = os.path.join(out_dir, \"summaries\", \"val\")\n",
    "\n",
    "# Checkpoint directory.\n",
    "checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "# 保存模型，最多保存5个模型\n",
    "saver = tf.train.Saver(tf.global_variables(), max_to_keep=MAX_CHECK_POINTS)\n",
    "\n",
    "# Write vocabulary\n",
    "vocab_processor.save(os.path.join(out_dir, \"vocab\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_batch, y_batch,train_summary_op,writer=None):\n",
    "    feed_dict = {\n",
    "      text_cnn.input_x: x_batch,\n",
    "      text_cnn.input_y: y_batch,\n",
    "      text_cnn.dropout_keep_prob: DROPOUT_KEEP_PROB\n",
    "    }\n",
    "    _, step, summaries, loss, accuracy = sess.run(\n",
    "        [train_op, global_step, train_summary_op, text_cnn.loss, text_cnn.accuracy],\n",
    "        feed_dict)\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    if (step%10==0):\n",
    "        print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "    if writer:\n",
    "        writer.add_summary(summaries, step)\n",
    "\n",
    "def val_step(x_batch, y_batch, val_summary_op,writer=None):\n",
    "    feed_dict = {\n",
    "      text_cnn.input_x: x_batch,\n",
    "      text_cnn.input_y: y_batch,\n",
    "      text_cnn.dropout_keep_prob: 1.0\n",
    "    }\n",
    "    step, summaries, loss, accuracy = sess.run(\n",
    "        [global_step, val_summary_op, text_cnn.loss, text_cnn.accuracy],\n",
    "        feed_dict)\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "    if writer:\n",
    "        writer.add_summary(summaries, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches_per_epoch: 5\n",
      "2019-06-12T16:12:43.867681: step 10, loss 1.72706, acc 0.522792\n",
      "2019-06-12T16:12:45.972055: step 20, loss 1.4794, acc 0.578348\n",
      "2019-06-12T16:12:48.092359: step 30, loss 1.35764, acc 0.567664\n",
      "2019-06-12T16:12:50.199751: step 40, loss 1.17878, acc 0.601852\n",
      "2019-06-12T16:12:52.328071: step 50, loss 1.03224, acc 0.621083\n",
      "2019-06-12T16:12:54.451358: step 60, loss 0.905689, acc 0.643162\n",
      "2019-06-12T16:12:56.568697: step 70, loss 0.847995, acc 0.671652\n",
      "2019-06-12T16:12:58.683072: step 80, loss 0.763915, acc 0.67735\n",
      "2019-06-12T16:13:00.818335: step 90, loss 0.75241, acc 0.685897\n",
      "2019-06-12T16:13:02.967618: step 100, loss 0.676618, acc 0.700855\n",
      "\n",
      "Evaluation:\n",
      "2019-06-12T16:13:03.140139: step 100, loss 0.623103, acc 0.652908\n",
      "2019-06-12T16:13:05.252513: step 110, loss 0.628261, acc 0.7151\n",
      "2019-06-12T16:13:07.393816: step 120, loss 0.575601, acc 0.749288\n",
      "2019-06-12T16:13:09.517529: step 130, loss 0.513088, acc 0.76567\n",
      "2019-06-12T16:13:11.628872: step 140, loss 0.485832, acc 0.770655\n",
      "2019-06-12T16:13:13.740213: step 150, loss 0.4752, acc 0.784188\n",
      "2019-06-12T16:13:15.856555: step 160, loss 0.462617, acc 0.782764\n",
      "2019-06-12T16:13:17.982898: step 170, loss 0.414102, acc 0.814815\n",
      "2019-06-12T16:13:20.085250: step 180, loss 0.388753, acc 0.831197\n",
      "2019-06-12T16:13:22.193612: step 190, loss 0.352968, acc 0.856125\n",
      "2019-06-12T16:13:24.309964: step 200, loss 0.40416, acc 0.814815\n",
      "\n",
      "Evaluation:\n",
      "2019-06-12T16:13:24.335885: step 200, loss 0.582539, acc 0.69137\n",
      "Saved model checkpoint to F:\\JupyterRoot\\DL_Parctise\\tensorflow_study\\14.cnn text classification\\runs\\1560327157\\checkpoints\\model-200\n",
      "\n",
      "2019-06-12T16:13:26.988987: step 210, loss 0.314285, acc 0.876781\n",
      "2019-06-12T16:13:29.044896: step 220, loss 0.313919, acc 0.866097\n",
      "2019-06-12T16:13:31.105586: step 230, loss 0.314801, acc 0.867521\n",
      "2019-06-12T16:13:33.159011: step 240, loss 0.295502, acc 0.874644\n",
      "2019-06-12T16:13:35.231591: step 250, loss 0.25594, acc 0.894587\n",
      "2019-06-12T16:13:37.295408: step 260, loss 0.264387, acc 0.894587\n",
      "2019-06-12T16:13:39.413471: step 270, loss 0.225343, acc 0.904558\n",
      "2019-06-12T16:13:41.605963: step 280, loss 0.214212, acc 0.910969\n",
      "2019-06-12T16:13:43.708258: step 290, loss 0.202617, acc 0.921652\n",
      "2019-06-12T16:13:45.831016: step 300, loss 0.200899, acc 0.923789\n",
      "\n",
      "Evaluation:\n",
      "2019-06-12T16:13:45.855923: step 300, loss 0.58769, acc 0.712008\n",
      "2019-06-12T16:13:47.992210: step 310, loss 0.167468, acc 0.942308\n",
      "2019-06-12T16:13:50.223245: step 320, loss 0.160165, acc 0.940171\n",
      "2019-06-12T16:13:52.363525: step 330, loss 0.150234, acc 0.942308\n",
      "2019-06-12T16:13:54.522752: step 340, loss 0.151625, acc 0.946581\n",
      "2019-06-12T16:13:56.663058: step 350, loss 0.133261, acc 0.957977\n",
      "2019-06-12T16:13:58.802336: step 360, loss 0.128923, acc 0.952991\n",
      "2019-06-12T16:14:01.010584: step 370, loss 0.120933, acc 0.962251\n",
      "2019-06-12T16:14:03.185365: step 380, loss 0.128034, acc 0.958689\n",
      "2019-06-12T16:14:05.393575: step 390, loss 0.0949935, acc 0.969373\n",
      "2019-06-12T16:14:07.564166: step 400, loss 0.0880174, acc 0.973647\n",
      "\n",
      "Evaluation:\n",
      "2019-06-12T16:14:07.590096: step 400, loss 0.64324, acc 0.717636\n",
      "Saved model checkpoint to F:\\JupyterRoot\\DL_Parctise\\tensorflow_study\\14.cnn text classification\\runs\\1560327157\\checkpoints\\model-400\n",
      "\n",
      "2019-06-12T16:14:10.290173: step 410, loss 0.0800659, acc 0.976496\n",
      "2019-06-12T16:14:12.438430: step 420, loss 0.0755611, acc 0.975071\n",
      "2019-06-12T16:14:14.613574: step 430, loss 0.0791108, acc 0.972934\n",
      "2019-06-12T16:14:16.763886: step 440, loss 0.0719137, acc 0.97792\n",
      "2019-06-12T16:14:18.907592: step 450, loss 0.0720047, acc 0.97792\n",
      "2019-06-12T16:14:21.075556: step 460, loss 0.0595275, acc 0.98433\n",
      "2019-06-12T16:14:23.287506: step 470, loss 0.0536186, acc 0.987892\n",
      "2019-06-12T16:14:25.484266: step 480, loss 0.0517645, acc 0.989316\n",
      "2019-06-12T16:14:27.634089: step 490, loss 0.0453239, acc 0.990741\n",
      "2019-06-12T16:14:29.775360: step 500, loss 0.0478061, acc 0.987179\n",
      "\n",
      "Evaluation:\n",
      "2019-06-12T16:14:29.801264: step 500, loss 0.730669, acc 0.721388\n",
      "2019-06-12T16:14:31.933562: step 510, loss 0.0341311, acc 0.994302\n",
      "2019-06-12T16:14:34.049933: step 520, loss 0.0386718, acc 0.992165\n",
      "2019-06-12T16:14:36.189771: step 530, loss 0.0325958, acc 0.992165\n",
      "2019-06-12T16:14:38.365980: step 540, loss 0.0387618, acc 0.991453\n",
      "2019-06-12T16:14:40.539460: step 550, loss 0.0301896, acc 0.99359\n",
      "2019-06-12T16:14:42.685722: step 560, loss 0.0311266, acc 0.99359\n",
      "2019-06-12T16:14:44.834977: step 570, loss 0.0250406, acc 0.996439\n",
      "2019-06-12T16:14:46.965310: step 580, loss 0.0185289, acc 0.997863\n",
      "2019-06-12T16:14:49.089168: step 590, loss 0.0277047, acc 0.995726\n",
      "2019-06-12T16:14:51.226640: step 600, loss 0.028955, acc 0.992877\n",
      "\n",
      "Evaluation:\n",
      "2019-06-12T16:14:51.252566: step 600, loss 0.821019, acc 0.733584\n",
      "Saved model checkpoint to F:\\JupyterRoot\\DL_Parctise\\tensorflow_study\\14.cnn text classification\\runs\\1560327157\\checkpoints\\model-600\n",
      "\n",
      "2019-06-12T16:14:53.856470: step 610, loss 0.0233518, acc 0.995014\n",
      "2019-06-12T16:14:56.043368: step 620, loss 0.0174964, acc 0.997151\n",
      "2019-06-12T16:14:58.202595: step 630, loss 0.021698, acc 0.996439\n",
      "2019-06-12T16:15:00.358830: step 640, loss 0.0196075, acc 0.997151\n",
      "2019-06-12T16:15:02.565147: step 650, loss 0.0166662, acc 0.996439\n",
      "2019-06-12T16:15:04.807405: step 660, loss 0.0193079, acc 0.997151\n",
      "2019-06-12T16:15:07.015928: step 670, loss 0.0179632, acc 0.997151\n",
      "2019-06-12T16:15:09.240599: step 680, loss 0.0135019, acc 0.999288\n",
      "2019-06-12T16:15:11.407773: step 690, loss 0.0164346, acc 0.997151\n",
      "2019-06-12T16:15:13.528104: step 700, loss 0.0123897, acc 0.999288\n",
      "\n",
      "Evaluation:\n",
      "2019-06-12T16:15:13.554034: step 700, loss 0.900653, acc 0.737336\n",
      "2019-06-12T16:15:15.682823: step 710, loss 0.0153037, acc 0.997151\n",
      "2019-06-12T16:15:17.805329: step 720, loss 0.0128848, acc 0.998576\n",
      "2019-06-12T16:15:19.957198: step 730, loss 0.0106986, acc 0.999288\n",
      "2019-06-12T16:15:22.114684: step 740, loss 0.0134245, acc 0.998576\n",
      "2019-06-12T16:15:24.399010: step 750, loss 0.0107272, acc 0.998576\n",
      "2019-06-12T16:15:26.587160: step 760, loss 0.0117948, acc 0.998576\n",
      "2019-06-12T16:15:28.780645: step 770, loss 0.0123166, acc 0.997151\n",
      "2019-06-12T16:15:30.990250: step 780, loss 0.0147362, acc 0.997151\n",
      "2019-06-12T16:15:33.160743: step 790, loss 0.0111288, acc 0.998576\n",
      "2019-06-12T16:15:35.335954: step 800, loss 0.0164139, acc 0.997151\n",
      "\n",
      "Evaluation:\n",
      "2019-06-12T16:15:35.362883: step 800, loss 0.969591, acc 0.738274\n",
      "WARNING:tensorflow:From F:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Saved model checkpoint to F:\\JupyterRoot\\DL_Parctise\\tensorflow_study\\14.cnn text classification\\runs\\1560327157\\checkpoints\\model-800\n",
      "\n",
      "2019-06-12T16:15:37.997434: step 810, loss 0.00873229, acc 0.998576\n",
      "2019-06-12T16:15:40.155538: step 820, loss 0.00880313, acc 0.999288\n",
      "2019-06-12T16:15:42.319996: step 830, loss 0.00777851, acc 1\n",
      "2019-06-12T16:15:44.481956: step 840, loss 0.0122574, acc 0.997151\n",
      "2019-06-12T16:15:46.647212: step 850, loss 0.00901789, acc 0.998576\n",
      "2019-06-12T16:15:48.820072: step 860, loss 0.00734775, acc 1\n",
      "2019-06-12T16:15:50.997158: step 870, loss 0.00713054, acc 0.999288\n",
      "2019-06-12T16:15:53.162875: step 880, loss 0.00587287, acc 1\n",
      "2019-06-12T16:15:55.336895: step 890, loss 0.00848867, acc 0.998576\n",
      "2019-06-12T16:15:57.517987: step 900, loss 0.00569725, acc 0.999288\n",
      "\n",
      "Evaluation:\n",
      "2019-06-12T16:15:57.545913: step 900, loss 1.02219, acc 0.737336\n",
      "2019-06-12T16:15:59.705304: step 910, loss 0.00552317, acc 1\n",
      "2019-06-12T16:16:01.919009: step 920, loss 0.00720414, acc 1\n",
      "2019-06-12T16:16:04.079188: step 930, loss 0.00609721, acc 0.998576\n",
      "2019-06-12T16:16:06.225573: step 940, loss 0.0067422, acc 0.999288\n",
      "2019-06-12T16:16:08.417659: step 950, loss 0.00492995, acc 1\n",
      "2019-06-12T16:16:10.566063: step 960, loss 0.00651509, acc 0.998576\n",
      "2019-06-12T16:16:12.738445: step 970, loss 0.00649453, acc 1\n",
      "2019-06-12T16:16:14.881226: step 980, loss 0.00610709, acc 0.999288\n",
      "2019-06-12T16:16:17.025190: step 990, loss 0.00573029, acc 0.999288\n",
      "2019-06-12T16:16:19.214073: step 1000, loss 0.00688411, acc 0.998576\n",
      "\n",
      "Evaluation:\n",
      "2019-06-12T16:16:19.240003: step 1000, loss 1.06725, acc 0.728893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to F:\\JupyterRoot\\DL_Parctise\\tensorflow_study\\14.cnn text classification\\runs\\1560327157\\checkpoints\\model-1000\n",
      "\n",
      "2019-06-12T16:16:21.864735: step 1010, loss 0.00514673, acc 1\n",
      "2019-06-12T16:16:24.032982: step 1020, loss 0.00456088, acc 1\n",
      "2019-06-12T16:16:26.166339: step 1030, loss 0.00580251, acc 1\n",
      "2019-06-12T16:16:28.335791: step 1040, loss 0.00550995, acc 0.999288\n",
      "2019-06-12T16:16:30.496500: step 1050, loss 0.00460873, acc 0.999288\n",
      "2019-06-12T16:16:32.645007: step 1060, loss 0.00483391, acc 0.998576\n",
      "2019-06-12T16:16:34.791228: step 1070, loss 0.00522646, acc 1\n",
      "2019-06-12T16:16:36.923164: step 1080, loss 0.00358192, acc 1\n",
      "2019-06-12T16:16:39.065479: step 1090, loss 0.00907051, acc 0.998576\n",
      "2019-06-12T16:16:41.232499: step 1100, loss 0.003951, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-06-12T16:16:41.258430: step 1100, loss 1.11464, acc 0.74015\n",
      "2019-06-12T16:16:43.389200: step 1110, loss 0.00442076, acc 0.999288\n",
      "2019-06-12T16:16:45.521725: step 1120, loss 0.00588906, acc 0.997863\n",
      "2019-06-12T16:16:47.653575: step 1130, loss 0.0036239, acc 1\n",
      "2019-06-12T16:16:49.774497: step 1140, loss 0.0039015, acc 0.999288\n",
      "2019-06-12T16:16:51.902428: step 1150, loss 0.00478305, acc 1\n",
      "2019-06-12T16:16:54.043841: step 1160, loss 0.00438011, acc 1\n",
      "2019-06-12T16:16:56.180561: step 1170, loss 0.00396981, acc 1\n",
      "2019-06-12T16:16:58.311822: step 1180, loss 0.00285273, acc 1\n",
      "2019-06-12T16:17:00.449798: step 1190, loss 0.00374151, acc 1\n",
      "2019-06-12T16:17:02.595763: step 1200, loss 0.00446907, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-06-12T16:17:02.622169: step 1200, loss 1.1526, acc 0.741088\n",
      "Saved model checkpoint to F:\\JupyterRoot\\DL_Parctise\\tensorflow_study\\14.cnn text classification\\runs\\1560327157\\checkpoints\\model-1200\n",
      "\n",
      "2019-06-12T16:17:05.203403: step 1210, loss 0.00465377, acc 0.999288\n",
      "2019-06-12T16:17:07.387472: step 1220, loss 0.00339089, acc 1\n",
      "2019-06-12T16:17:09.528356: step 1230, loss 0.00359553, acc 0.999288\n",
      "2019-06-12T16:17:11.655938: step 1240, loss 0.00377446, acc 0.999288\n",
      "2019-06-12T16:17:13.781883: step 1250, loss 0.00349974, acc 1\n",
      "2019-06-12T16:17:15.905066: step 1260, loss 0.00428576, acc 0.998576\n",
      "2019-06-12T16:17:18.022008: step 1270, loss 0.00317581, acc 1\n",
      "2019-06-12T16:17:20.143849: step 1280, loss 0.00383864, acc 0.999288\n",
      "2019-06-12T16:17:22.286335: step 1290, loss 0.00272662, acc 1\n",
      "2019-06-12T16:17:24.432049: step 1300, loss 0.003854, acc 0.999288\n",
      "\n",
      "Evaluation:\n",
      "2019-06-12T16:17:24.457980: step 1300, loss 1.20314, acc 0.732645\n",
      "2019-06-12T16:17:26.596475: step 1310, loss 0.00337497, acc 1\n",
      "2019-06-12T16:17:28.737486: step 1320, loss 0.0042419, acc 0.999288\n",
      "2019-06-12T16:17:30.873387: step 1330, loss 0.00295003, acc 1\n",
      "2019-06-12T16:17:33.000960: step 1340, loss 0.00329697, acc 1\n",
      "2019-06-12T16:17:35.134985: step 1350, loss 0.00361908, acc 0.999288\n",
      "2019-06-12T16:17:37.260055: step 1360, loss 0.00349765, acc 1\n",
      "2019-06-12T16:17:39.396148: step 1370, loss 0.0024504, acc 1\n",
      "2019-06-12T16:17:41.536868: step 1380, loss 0.00299849, acc 1\n",
      "2019-06-12T16:17:43.669367: step 1390, loss 0.00549273, acc 0.998576\n",
      "2019-06-12T16:17:45.802250: step 1400, loss 0.00311956, acc 0.999288\n",
      "\n",
      "Evaluation:\n",
      "2019-06-12T16:17:45.828151: step 1400, loss 1.24169, acc 0.730769\n",
      "Saved model checkpoint to F:\\JupyterRoot\\DL_Parctise\\tensorflow_study\\14.cnn text classification\\runs\\1560327157\\checkpoints\\model-1400\n",
      "\n",
      "2019-06-12T16:17:48.475219: step 1410, loss 0.00308734, acc 1\n",
      "2019-06-12T16:17:50.636471: step 1420, loss 0.00414165, acc 0.999288\n",
      "2019-06-12T16:17:52.801524: step 1430, loss 0.00259129, acc 1\n",
      "2019-06-12T16:17:54.953194: step 1440, loss 0.00252955, acc 1\n",
      "2019-06-12T16:17:57.038067: step 1450, loss 0.00265621, acc 1\n",
      "2019-06-12T16:17:59.107013: step 1460, loss 0.00372943, acc 1\n",
      "2019-06-12T16:18:01.198866: step 1470, loss 0.00232228, acc 1\n",
      "2019-06-12T16:18:03.295489: step 1480, loss 0.00253256, acc 1\n",
      "2019-06-12T16:18:05.395859: step 1490, loss 0.00265594, acc 1\n",
      "2019-06-12T16:18:07.517613: step 1500, loss 0.00246337, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2019-06-12T16:18:07.544512: step 1500, loss 1.2667, acc 0.733584\n"
     ]
    }
   ],
   "source": [
    "session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=False,\n",
    "      log_device_placement=False)\n",
    "with tf.Session(config=session_conf) as sess:\n",
    "    with tf.summary.FileWriter(train_summary_dir, sess.graph) as train_summary_writer:\n",
    "        with tf.summary.FileWriter(val_summary_dir, sess.graph) as val_summary_writer:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            # Generate batches\n",
    "            batches = data_helpers.batch_iter(\n",
    "                list(zip(x_train, y_train)), BATCH_SIZE, NUM_EPOCH)\n",
    "            # Training loop. For each batch...\n",
    "            for batch in batches:\n",
    "                x_batch, y_batch = zip(*batch)\n",
    "                train_step(x_batch, y_batch,train_summary_op,train_summary_writer)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                # 测试\n",
    "                if current_step % EVAL_STEPS == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    val_step(x_val, y_val,val_summary_op, writer=val_summary_writer)\n",
    "                # 保存模型\n",
    "                if current_step % CHECKPOINT_STEPS == 0:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
