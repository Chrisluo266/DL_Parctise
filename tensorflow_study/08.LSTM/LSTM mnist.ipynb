{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-3d8a9d232fa5>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From F:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From F:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From F:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From F:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From F:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From F:\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#普通神经元weights\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape,stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "#普通神经元bias\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1,shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "def LSTMLayer(input,unitNum,outputDimen,keep_prob):\n",
    "    #input shape is [batchsize,vector的最大长度,数据个数]\n",
    "    weight= weight_variable([unitNum,outputDimen])\n",
    "    bians = bias_variable([outputDimen]) \n",
    "    lstn_cell = tf.contrib.rnn.BasicLSTMCell(unitNum)\n",
    "    #final_state[0] 维度=[batch_size,unitNum] is 最终存的Cell值\n",
    "    #final_state[1] 维度=[batch_size,unitNum] is 最终的输出值\n",
    "    #outputs 维度=[batch_size,time_steps,unitNum]如果time_major=False\n",
    "    #outputs 维度=[time_steps,batch_size,unitNum]如果time_major=True\n",
    "    #outputs记录了每一个批次的数据的每一次过Cell的最终输出值\n",
    "    outputs,final_state = tf.nn.dynamic_rnn(lstn_cell,input,dtype=tf.float32)\n",
    "    result = tf.matmul(final_state[1],weight) + bians\n",
    "    output = tf.nn.dropout(result,keep_prob)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "n_inputs = 28 #一行28个数据\n",
    "time_steps = 28 #一共28行\n",
    "batch_size = 1024\n",
    "n_batch = math.ceil(mnist.train.num_examples / batch_size)\n",
    "max_steps = 30\n",
    "\n",
    "with tf.name_scope(\"Input\"):\n",
    "    x = tf.placeholder(tf.float32,[None,784],name='x') #28*28=784\n",
    "    y = tf.placeholder(tf.float32,[None,10],name='y') #28*28=784\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "reshape_x = tf.reshape(x,[-1,time_steps,n_inputs])\n",
    "#LSTM层 100个\n",
    "with tf.name_scope(\"LSTM_layer1\"):\n",
    "    lstmResult = LSTMLayer(reshape_x,100,10,keep_prob)\n",
    "\n",
    "#loss\n",
    "with tf.name_scope(\"Loss\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y,logits=lstmResult))\n",
    "    tf.summary.scalar(\"Loss\",loss)\n",
    "\n",
    "#train\n",
    "with tf.name_scope(\"Train\"):\n",
    "    train_step = tf.train.AdamOptimizer(0.0001).minimize(loss)\n",
    "\n",
    "#accuracy\n",
    "with tf.name_scope(\"Accuracy\"):\n",
    "    predictionIndex = tf.argmax(tf.nn.softmax(lstmResult),1)\n",
    "    realIndex = tf.argmax(y,1)\n",
    "    correctBools = tf.equal(predictionIndex,realIndex)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correctBools,tf.float32))\n",
    "    tf.summary.scalar(\"Accuracy\",accuracy)\n",
    "        \n",
    "\n",
    "summaryMerge = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Train.\n",
      "Iter = 0, Testing Accuracy = 0.321399986743927,  TrainTime = 2.8953825039998264\n",
      "Iter = 1, Testing Accuracy = 0.3684999942779541,  TrainTime = 2.5928150509998886\n",
      "Iter = 2, Testing Accuracy = 0.5027999877929688,  TrainTime = 2.559751378999863\n",
      "Iter = 3, Testing Accuracy = 0.6407999992370605,  TrainTime = 2.521129020000444\n",
      "Iter = 4, Testing Accuracy = 0.7325999736785889,  TrainTime = 2.574146577000647\n",
      "Iter = 5, Testing Accuracy = 0.7856000065803528,  TrainTime = 2.5101997399997344\n",
      "Iter = 6, Testing Accuracy = 0.821399986743927,  TrainTime = 2.5867572850002034\n",
      "Iter = 7, Testing Accuracy = 0.8424000144004822,  TrainTime = 2.581100643999889\n",
      "Iter = 8, Testing Accuracy = 0.8592000007629395,  TrainTime = 2.5293216230002145\n",
      "Iter = 9, Testing Accuracy = 0.8715000152587891,  TrainTime = 2.505403344000115\n",
      "Iter = 10, Testing Accuracy = 0.8806999921798706,  TrainTime = 2.5289304130001256\n",
      "Iter = 11, Testing Accuracy = 0.8894000053405762,  TrainTime = 2.4878997900004833\n",
      "Iter = 12, Testing Accuracy = 0.8931999802589417,  TrainTime = 2.458909584000139\n",
      "Iter = 13, Testing Accuracy = 0.9024999737739563,  TrainTime = 2.4644436329999735\n",
      "Iter = 14, Testing Accuracy = 0.9079999923706055,  TrainTime = 2.456064437999885\n",
      "Iter = 15, Testing Accuracy = 0.9108999967575073,  TrainTime = 2.428746345000036\n",
      "Iter = 16, Testing Accuracy = 0.9157999753952026,  TrainTime = 2.4595919499997763\n",
      "Iter = 17, Testing Accuracy = 0.9168000221252441,  TrainTime = 2.5817124000004696\n",
      "Iter = 18, Testing Accuracy = 0.9210000038146973,  TrainTime = 3.5190963439999905\n",
      "Iter = 19, Testing Accuracy = 0.9236000180244446,  TrainTime = 2.930465630999606\n",
      "Iter = 20, Testing Accuracy = 0.9262999892234802,  TrainTime = 2.7429422699997303\n",
      "Iter = 21, Testing Accuracy = 0.930899977684021,  TrainTime = 2.787702020999859\n",
      "Iter = 22, Testing Accuracy = 0.9298999905586243,  TrainTime = 2.79397913000048\n",
      "Iter = 23, Testing Accuracy = 0.9340000152587891,  TrainTime = 2.704746578000595\n",
      "Iter = 24, Testing Accuracy = 0.9337999820709229,  TrainTime = 2.830029771000227\n",
      "Iter = 25, Testing Accuracy = 0.9363999962806702,  TrainTime = 3.258145128999786\n",
      "Iter = 26, Testing Accuracy = 0.9383999705314636,  TrainTime = 3.110457122000298\n",
      "Iter = 27, Testing Accuracy = 0.9377999901771545,  TrainTime = 4.05326754699945\n",
      "Iter = 28, Testing Accuracy = 0.9424999952316284,  TrainTime = 3.353923111999393\n",
      "Iter = 29, Testing Accuracy = 0.9423999786376953,  TrainTime = 4.048076934999699\n",
      "End Train.   Use Time =  83.86861396999939\n"
     ]
    }
   ],
   "source": [
    "trainlog_path = \"./logs\\\\train\\\\\"\n",
    "testlog_path= \"./logs\\\\test\\\\\"\n",
    "\n",
    "#用来保存网络模型\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    with tf.summary.FileWriter(trainlog_path,sess.graph) as trainWriter:\n",
    "        with tf.summary.FileWriter(testlog_path,sess.graph) as testWriter:\n",
    "            startClock = time.perf_counter()\n",
    "            print(\"Start Train.\")\n",
    "            for epoch in range(max_steps):\n",
    "                startEpochTime = time.perf_counter()\n",
    "                for batch in range(n_batch):\n",
    "                    batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
    "                    trainResult,summary = sess.run([train_step,summaryMerge],feed_dict={x:batch_xs,y:batch_ys,keep_prob:1})\n",
    "                    trainWriter.add_summary(summary,epoch)\n",
    "                    batch_xs,batch_ys = mnist.test.next_batch(batch_size)\n",
    "                    summary = sess.run(summaryMerge,feed_dict={x:batch_xs,y:batch_ys,keep_prob:1})\n",
    "                    testWriter.add_summary(summary,epoch)\n",
    "                acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_prob:1})\n",
    "                print(\"Iter = {0}, Testing Accuracy = {1},  TrainTime = {2}\".format(epoch,acc,time.perf_counter() - startEpochTime))\n",
    "            print(\"End Train.   Use Time = \",time.perf_counter() - startClock)\n",
    "        #saver.save(sess, path_for_savedata, global_step=max_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
